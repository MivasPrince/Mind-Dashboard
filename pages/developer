"""
Developer Dashboard - Telemetry & Observability
System health, AI usage, performance monitoring, error tracking
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import streamlit as st
import pandas as pd
from core.db import get_bigquery_client, run_query
from core.settings import get_table_ref
from components.ui import (
    render_kpi_row, render_line_chart, render_bar_chart, render_heatmap,
    render_pie_chart, render_data_table, render_global_filters,
    render_alert_card, render_data_unavailable, format_number
)

def render():
    """Render the enhanced developer dashboard"""
    
    st.markdown('<h1 class="main-header">‚öôÔ∏è Developer Dashboard</h1>', unsafe_allow_html=True)
    st.write("**System Health & Observability** - Backend telemetry and AI monitoring")
    
    # Get BigQuery client
    client = get_bigquery_client()
    if client is None:
        st.error("Failed to connect to BigQuery")
        st.stop()
    
    # Time Range Selector
    time_range = st.selectbox(
        "Time Range",
        ["Last Hour", "Last 24 Hours", "Last 7 Days", "Last 30 Days"],
        index=1
    )
    
    time_mapping = {
        "Last Hour": "1 HOUR",
        "Last 24 Hours": "1 DAY",
        "Last 7 Days": "7 DAY",
        "Last 30 Days": "30 DAY"
    }
    
    time_interval = time_mapping[time_range]
    
    st.divider()
    
    # ========================================================================
    # A. SYSTEM HEALTH KPI CARDS
    # ========================================================================
    
    st.subheader("üìä System Health Metrics")
    
    with st.spinner("Loading system metrics..."):
        # Total Requests
        total_req_query = f"""
        SELECT COUNT(*) as total_requests
        FROM {get_table_ref('backend_telemetry')}
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
        """
        total_req_df = run_query(total_req_query, client)
        total_requests = total_req_df['total_requests'].iloc[0] if total_req_df is not None and not total_req_df.empty else 0
        
        # Average Response Time
        avg_resp_query = f"""
        SELECT AVG(derived_response_time_ms) as avg_response
        FROM {get_table_ref('backend_telemetry')}
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
        AND derived_response_time_ms IS NOT NULL
        """
        avg_resp_df = run_query(avg_resp_query, client)
        avg_response = avg_resp_df['avg_response'].iloc[0] if avg_resp_df is not None and not avg_resp_df.empty else 0
        
        # P95 Response Time
        p95_query = f"""
        SELECT APPROX_QUANTILES(derived_response_time_ms, 100)[OFFSET(95)] as p95
        FROM {get_table_ref('backend_telemetry')}
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
        AND derived_response_time_ms IS NOT NULL
        """
        p95_df = run_query(p95_query, client)
        p95 = p95_df['p95'].iloc[0] if p95_df is not None and not p95_df.empty else 0
        
        # Error Count
        error_query = f"""
        SELECT COUNT(*) as error_count
        FROM {get_table_ref('backend_telemetry')}
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
        AND derived_is_error = TRUE
        """
        error_df = run_query(error_query, client)
        error_count = error_df['error_count'].iloc[0] if error_df is not None and not error_df.empty else 0
        
        # Error Rate
        error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0
        
        # AI Token Usage
        token_query = f"""
        SELECT SUM(derived_ai_total_tokens) as total_tokens
        FROM {get_table_ref('backend_telemetry')}
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
        AND derived_ai_total_tokens IS NOT NULL
        """
        token_df = run_query(token_query, client)
        total_tokens = token_df['total_tokens'].iloc[0] if token_df is not None and not token_df.empty else 0
        
        # Cost Estimate (assuming $0.002 per 1K tokens)
        estimated_cost = (total_tokens / 1000) * 0.002 if total_tokens else 0
    
    render_kpi_row([
        {'title': 'Total Requests', 'value': format_number(total_requests), 'icon': 'üì°',
         'help_text': f'Total requests in {time_range.lower()}'},
        {'title': 'Avg Response Time', 'value': f"{avg_response:.0f} ms", 'icon': '‚è±Ô∏è',
         'help_text': 'Average response time'},
        {'title': 'P95 Response Time', 'value': f"{p95:.0f} ms", 'icon': 'üìà',
         'help_text': '95th percentile response time'},
        {'title': 'Error Rate', 'value': f"{error_rate:.2f}%", 'icon': '‚ö†Ô∏è',
         'help_text': 'Percentage of failed requests'}
    ])
    
    render_kpi_row([
        {'title': 'Error Count', 'value': format_number(error_count), 'icon': '‚ùå',
         'help_text': f'Total errors in {time_range.lower()}'},
        {'title': 'AI Tokens Used', 'value': format_number(total_tokens), 'icon': 'ü§ñ',
         'help_text': 'Total tokens consumed by AI'},
        {'title': 'Estimated Cost', 'value': f"${estimated_cost:.2f}", 'icon': 'üí∞',
         'help_text': 'Estimated AI cost (GPT-4 rates)'},
        {'title': 'System Status', 'value': '‚úÖ Healthy' if error_rate < 5 else '‚ö†Ô∏è Issues', 'icon': 'üè•',
         'help_text': 'Overall system health'}
    ])
    
    st.divider()
    
    # ========================================================================
    # B. BACKEND TELEMETRY - RESPONSE TIME
    # ========================================================================
    
    st.subheader("‚è±Ô∏è Response Time Monitoring")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Response Time Over Time
        resp_time_query = f"""
        SELECT 
            TIMESTAMP_TRUNC(created_at, HOUR) as hour,
            AVG(derived_response_time_ms) as avg_response,
            APPROX_QUANTILES(derived_response_time_ms, 100)[OFFSET(95)] as p95
        FROM {get_table_ref('backend_telemetry')}
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
        AND derived_response_time_ms IS NOT NULL
        GROUP BY hour
        ORDER BY hour
        """
        resp_time_data = run_query(resp_time_query, client)
        
        if resp_time_data is not None and not resp_time_data.empty:
            render_line_chart(resp_time_data, 'hour', 'avg_response',
                            'Average Response Time Over Time')
        else:
            render_data_unavailable()
    
    with col2:
        # Top Slow Endpoints
        slow_endpoints_query = f"""
        SELECT 
            derived_endpoint_group as endpoint,
            AVG(derived_response_time_ms) as avg_response,
            COUNT(*) as request_count
        FROM {get_table_ref('backend_telemetry')}
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
        AND derived_response_time_ms IS NOT NULL
        AND derived_endpoint_group IS NOT NULL
        GROUP BY endpoint
        ORDER BY avg_response DESC
        LIMIT 10
        """
        slow_endpoints = run_query(slow_endpoints_query, client)
        
        if slow_endpoints is not None and not slow_endpoints.empty:
            render_bar_chart(slow_endpoints, 'endpoint', 'avg_response',
                           'Slowest Endpoints', orientation='h')
        else:
            render_data_unavailable()
    
    st.divider()
    
    # ========================================================================
    # C. ERROR MONITORING
    # ========================================================================
    
    st.subheader("‚ùå Error Tracking & Analysis")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Error Trend
        error_trend_query = f"""
        SELECT 
            TIMESTAMP_TRUNC(created_at, HOUR) as hour,
            COUNTIF(derived_is_error = TRUE) as error_count,
            COUNT(*) as total_count,
            COUNTIF(derived_is_error = TRUE) * 100.0 / COUNT(*) as error_rate
        FROM {get_table_ref('backend_telemetry')}
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
        GROUP BY hour
        ORDER BY hour
        """
        error_trend = run_query(error_trend_query, client)
        
        if error_trend is not None and not error_trend.empty:
            render_line_chart(error_trend, 'hour', 'error_rate',
                            'Error Rate Over Time (%)')
        else:
            render_data_unavailable()
    
    with col2:
        # Errors by Endpoint
        error_endpoint_query = f"""
        SELECT 
            derived_endpoint_group as endpoint,
            COUNT(*) as error_count
        FROM {get_table_ref('backend_telemetry')}
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
        AND derived_is_error = TRUE
        AND derived_endpoint_group IS NOT NULL
        GROUP BY endpoint
        ORDER BY error_count DESC
        """
        error_endpoint = run_query(error_endpoint_query, client)
        
        if error_endpoint is not None and not error_endpoint.empty:
            render_pie_chart(error_endpoint, 'error_count', 'endpoint',
                           'Errors by Endpoint')
        else:
            render_data_unavailable()
    
    # Error Log Table
    st.markdown("**üîç Recent Errors**")
    error_log_query = f"""
    SELECT 
        created_at,
        span_name,
        derived_endpoint_group as endpoint,
        http_status_code,
        derived_response_time_ms as response_time,
        trace_id
    FROM {get_table_ref('backend_telemetry')}
    WHERE derived_is_error = TRUE
    AND created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
    ORDER BY created_at DESC
    LIMIT 100
    """
    error_log = run_query(error_log_query, client)
    
    if error_log is not None and not error_log.empty:
        render_data_table(error_log, "Error Log", max_rows=50, key_suffix="errors")
    else:
        st.success("‚úÖ No errors in selected time range!")
    
    st.divider()
    
    # ========================================================================
    # D. AI USAGE MONITORING
    # ========================================================================
    
    st.subheader("ü§ñ AI Usage & Cost Analysis")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Tokens Over Time
        tokens_time_query = f"""
        SELECT 
            DATE(created_at) as date,
            SUM(derived_ai_input_tokens) as input_tokens,
            SUM(derived_ai_output_tokens) as output_tokens,
            SUM(derived_ai_total_tokens) as total_tokens
        FROM {get_table_ref('backend_telemetry')}
        WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
        AND derived_ai_total_tokens IS NOT NULL
        GROUP BY date
        ORDER BY date
        """
        tokens_time = run_query(tokens_time_query, client)
        
        if tokens_time is not None and not tokens_time.empty:
            render_line_chart(tokens_time, 'date', 'total_tokens',
                            'AI Token Usage Over Time')
        else:
            render_data_unavailable()
    
    with col2:
        # Input vs Output Tokens
        if tokens_time is not None and not tokens_time.empty:
            total_input = tokens_time['input_tokens'].sum()
            total_output = tokens_time['output_tokens'].sum()
            
            token_dist = pd.DataFrame({
                'type': ['Input Tokens', 'Output Tokens'],
                'count': [total_input, total_output]
            })
            
            render_pie_chart(token_dist, 'count', 'type',
                           'Input vs Output Token Distribution')
        else:
            render_data_unavailable()
    
    # AI Usage Table
    st.markdown("**üìä AI Request Details**")
    ai_requests_query = f"""
    SELECT 
        created_at,
        span_name,
        derived_ai_input_tokens as input_tokens,
        derived_ai_output_tokens as output_tokens,
        derived_ai_total_tokens as total_tokens,
        derived_response_time_ms as response_time,
        derived_request_success as success
    FROM {get_table_ref('backend_telemetry')}
    WHERE derived_ai_total_tokens IS NOT NULL
    AND created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {time_interval})
    ORDER BY created_at DESC
    LIMIT 100
    """
    ai_requests = run_query(ai_requests_query, client)
    
    if ai_requests is not None and not ai_requests.empty:
        render_data_table(ai_requests, "AI Request Log", max_rows=50, key_suffix="ai")
    else:
        render_data_unavailable()
    
    st.divider()
    
    # ========================================================================
    # E. DATA QUALITY CHECKS
    # ========================================================================
    
    st.subheader("üîç Data Quality Monitoring")
    
    quality_checks = []
    
    # Check for null emails
    null_email_query = f"""
    SELECT 'Users without Email' as check_name, COUNT(*) as count
    FROM {get_table_ref('user')}
    WHERE email IS NULL
    """
    null_email = run_query(null_email_query, client)
    if null_email is not None:
        quality_checks.append(null_email.iloc[0].to_dict())
    
    # Check for orphaned conversations
    orphan_query = f"""
    SELECT 'Conversations without Grades' as check_name, COUNT(*) as count
    FROM {get_table_ref('conversation')} c
    LEFT JOIN {get_table_ref('grades')} g ON c.conversation_id = g.conversation_id
    WHERE g._id IS NULL
    """
    orphan = run_query(orphan_query, client)
    if orphan is not None:
        quality_checks.append(orphan.iloc[0].to_dict())
    
    if quality_checks:
        quality_df = pd.DataFrame(quality_checks)
        render_data_table(quality_df, "Data Quality Checks", max_rows=10, key_suffix="quality")
    
    st.divider()
    
    # System Health Summary
    st.subheader("üè• System Health Summary")
    
    if error_rate < 1:
        render_alert_card("System Status: Excellent", 
                         f"Error rate is {error_rate:.2f}% - well below threshold",
                         severity="success")
    elif error_rate < 5:
        render_alert_card("System Status: Good",
                         f"Error rate is {error_rate:.2f}% - within acceptable range",
                         severity="info")
    else:
        render_alert_card("System Status: Attention Required",
                         f"Error rate is {error_rate:.2f}% - above threshold, investigation recommended",
                         severity="warning")

if __name__ == "__main__":
    render()
