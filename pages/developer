"""
Developer Dashboard Page
System monitoring, data quality, and technical metrics
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import streamlit as st
from core.db import get_bigquery_client, run_query, get_available_tables, check_table_exists
from core.settings import get_table_ref, TABLES
from components.ui import (
    render_kpi_row, render_bar_chart, render_line_chart, 
    render_data_table, render_data_unavailable
)
import pandas as pd

def render():
    """Render the developer dashboard page"""
    
    st.markdown('<h1 class="main-header">üíª Developer Dashboard</h1>', unsafe_allow_html=True)
    st.write("System Monitoring & Data Quality")
    
    # Get BigQuery client
    client = get_bigquery_client()
    if client is None:
        st.error("Failed to connect to BigQuery")
        st.stop()
    
    # Data Freshness Check
    st.subheader("üïí Data Freshness")
    
    with st.spinner("Checking data freshness..."):
        freshness_checks = []
        
        for table_key, table_info in TABLES.items():
            table_name = table_info['name']
            
            if not check_table_exists(table_name, client):
                freshness_checks.append({
                    'table': table_info['display_name'],
                    'status': '‚ùå Not Found',
                    'last_update': 'N/A',
                    'row_count': 0
                })
                continue
            
            # Get last update time and row count
            query = f"""
            SELECT 
                COUNT(*) as row_count,
                MAX(GREATEST(
                    COALESCE(date_updated, TIMESTAMP('1970-01-01')),
                    COALESCE(date_added, TIMESTAMP('1970-01-01')),
                    COALESCE(timestamp, TIMESTAMP('1970-01-01')),
                    COALESCE(created_at, TIMESTAMP('1970-01-01'))
                )) as last_update
            FROM {get_table_ref(table_name)}
            """
            
            try:
                result = run_query(query, client)
                if result is not None and not result.empty:
                    row_count = result['row_count'].iloc[0]
                    last_update = result['last_update'].iloc[0]
                    
                    # Calculate freshness status
                    if pd.notna(last_update):
                        hours_old = (pd.Timestamp.now() - pd.Timestamp(last_update)).total_seconds() / 3600
                        if hours_old < 24:
                            status = '‚úÖ Fresh'
                        elif hours_old < 168:  # 1 week
                            status = '‚ö†Ô∏è Stale'
                        else:
                            status = '‚ùå Very Old'
                    else:
                        status = '‚ö†Ô∏è Unknown'
                        last_update = 'N/A'
                    
                    freshness_checks.append({
                        'table': table_info['display_name'],
                        'status': status,
                        'last_update': str(last_update) if pd.notna(last_update) else 'N/A',
                        'row_count': row_count
                    })
                else:
                    freshness_checks.append({
                        'table': table_info['display_name'],
                        'status': '‚ö†Ô∏è No Data',
                        'last_update': 'N/A',
                        'row_count': 0
                    })
            except:
                freshness_checks.append({
                    'table': table_info['display_name'],
                    'status': '‚ùå Query Failed',
                    'last_update': 'N/A',
                    'row_count': 0
                })
        
        freshness_df = pd.DataFrame(freshness_checks)
        st.dataframe(freshness_df, use_container_width=True, hide_index=True)
    
    st.divider()
    
    # System Health KPIs
    st.subheader("üìä System Health")
    
    with st.spinner("Loading system metrics..."):
        # Total records across all tables
        total_records = freshness_df['row_count'].sum()
        
        # Tables with data
        tables_with_data = len([x for x in freshness_checks if x['row_count'] > 0])
        
        # Fresh tables (updated in last 24h)
        fresh_tables = len([x for x in freshness_checks if '‚úÖ' in x['status']])
        
        # Backend errors (if telemetry available)
        error_query = f"""
        SELECT COUNT(*) as error_count
        FROM {get_table_ref('backend_telemetry')}
        WHERE derived_is_error = TRUE
        AND created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
        """
        try:
            error_df = run_query(error_query, client)
            error_count = error_df['error_count'].iloc[0] if error_df is not None and not error_df.empty else 0
        except:
            error_count = 0
    
    # Display KPIs
    render_kpi_row([
        {
            'title': 'Total Records',
            'value': f"{total_records:,}",
            'icon': 'üìä',
            'help_text': 'Total records across all tables'
        },
        {
            'title': 'Tables with Data',
            'value': f"{tables_with_data}/{len(TABLES)}",
            'icon': 'üìÅ',
            'help_text': 'Tables containing data'
        },
        {
            'title': 'Fresh Tables',
            'value': f"{fresh_tables}/{len(TABLES)}",
            'icon': '‚úÖ',
            'help_text': 'Tables updated in last 24 hours'
        },
        {
            'title': 'Errors (24h)',
            'value': f"{error_count:,}",
            'icon': '‚ö†Ô∏è',
            'help_text': 'Backend errors in last 24 hours'
        }
    ])
    
    st.divider()
    
    # Backend Telemetry Analysis
    st.subheader("üîç Backend Telemetry")
    
    if check_table_exists('backend_telemetry', client):
        col1, col2 = st.columns(2)
        
        with col1:
            # Response time distribution
            response_time_query = f"""
            SELECT 
                derived_endpoint_group as endpoint,
                AVG(derived_response_time_ms) as avg_response_time,
                MAX(derived_response_time_ms) as max_response_time
            FROM {get_table_ref('backend_telemetry')}
            WHERE derived_response_time_ms IS NOT NULL
            AND created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
            GROUP BY derived_endpoint_group
            ORDER BY avg_response_time DESC
            """
            response_time = run_query(response_time_query, client)
            
            if response_time is not None and not response_time.empty:
                render_bar_chart(
                    response_time,
                    x='endpoint',
                    y='avg_response_time',
                    title='Avg Response Time by Endpoint (ms)'
                )
            else:
                st.info("No response time data available")
        
        with col2:
            # Error rate by endpoint
            error_rate_query = f"""
            SELECT 
                derived_endpoint_group as endpoint,
                COUNT(*) as total_requests,
                SUM(CASE WHEN derived_is_error THEN 1 ELSE 0 END) as errors,
                ROUND(100.0 * SUM(CASE WHEN derived_is_error THEN 1 ELSE 0 END) / COUNT(*), 2) as error_rate
            FROM {get_table_ref('backend_telemetry')}
            WHERE created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
            GROUP BY derived_endpoint_group
            ORDER BY error_rate DESC
            """
            error_rate = run_query(error_rate_query, client)
            
            if error_rate is not None and not error_rate.empty:
                render_bar_chart(
                    error_rate,
                    x='endpoint',
                    y='error_rate',
                    title='Error Rate by Endpoint (%)'
                )
            else:
                st.info("No error rate data available")
        
        # AI Usage Metrics
        st.markdown("**ü§ñ AI Usage Metrics**")
        
        ai_usage_query = f"""
        SELECT 
            DATE(created_at) as date,
            COUNT(*) as ai_requests,
            SUM(derived_ai_total_tokens) as total_tokens,
            AVG(derived_ai_total_tokens) as avg_tokens_per_request
        FROM {get_table_ref('backend_telemetry')}
        WHERE derived_ai_total_tokens IS NOT NULL
        AND created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY)
        GROUP BY DATE(created_at)
        ORDER BY date DESC
        """
        ai_usage = run_query(ai_usage_query, client)
        
        if ai_usage is not None and not ai_usage.empty:
            col1, col2 = st.columns(2)
            
            with col1:
                render_line_chart(
                    ai_usage,
                    x='date',
                    y='ai_requests',
                    title='AI Requests Over Time'
                )
            
            with col2:
                render_line_chart(
                    ai_usage,
                    x='date',
                    y='total_tokens',
                    title='AI Tokens Consumed Over Time'
                )
            
            # AI model distribution
            model_dist_query = f"""
            SELECT 
                derived_ai_model as model,
                COUNT(*) as requests,
                SUM(derived_ai_total_tokens) as total_tokens
            FROM {get_table_ref('backend_telemetry')}
            WHERE derived_ai_model IS NOT NULL
            AND created_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
            GROUP BY derived_ai_model
            ORDER BY requests DESC
            """
            model_dist = run_query(model_dist_query, client)
            
            if model_dist is not None and not model_dist.empty:
                render_data_table(
                    model_dist,
                    title="AI Model Usage (Last 7 Days)",
                    max_rows=20
                )
        else:
            st.info("No AI usage data available")
    
    else:
        render_data_unavailable("backend_telemetry")
    
    st.divider()
    
    # Data Quality Checks
    st.subheader("‚úÖ Data Quality Checks")
    
    quality_checks = []
    
    # Check for orphaned records
    orphan_check_query = f"""
    SELECT 
        'Conversations without Grades' as check_name,
        COUNT(*) as count
    FROM {get_table_ref('conversation')} c
    LEFT JOIN {get_table_ref('grades')} g ON c.conversation_id = g.conversation_id
    WHERE g.grade_id IS NULL
    """
    orphan_result = run_query(orphan_check_query, client)
    if orphan_result is not None and not orphan_result.empty:
        quality_checks.append(orphan_result.iloc[0].to_dict())
    
    # Check for null critical fields
    null_check_query = f"""
    SELECT 
        'Users without Email' as check_name,
        COUNT(*) as count
    FROM {get_table_ref('user')}
    WHERE email IS NULL
    """
    null_result = run_query(null_check_query, client)
    if null_result is not None and not null_result.empty:
        quality_checks.append(null_result.iloc[0].to_dict())
    
    if quality_checks:
        quality_df = pd.DataFrame(quality_checks)
        st.dataframe(quality_df, use_container_width=True, hide_index=True)
    else:
        st.success("‚úÖ All data quality checks passed")
